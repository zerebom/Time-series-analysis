{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このコードの目的\n",
    "\n",
    "支持率が伸びている/落ちているデータセットをサイズが同じになるように集積して、  \n",
    "特徴分析を行う  \n",
    "データセットが同じになるようにするとき、前処理した後のtxtファイルで比較するほうがよい\n",
    "+  \n",
    "機械学習にかける  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from 過去のコード.Mecab_func import *\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各フォルダのサイズを比較するコード\n",
    "#普通にエクスプローラーで見ればいい説\n",
    "\n",
    "\n",
    "\n",
    "def compare_datasize(folder_A,folder_B):\n",
    "    file_list_A=glob.glob(folder_A)\n",
    "    file_list_B=glob.glob(folder_B)    \n",
    "    A_size=[]\n",
    "    B_size=[]\n",
    "    \n",
    "    for file_A in file_list_A:\n",
    "        A_size.append(os.path.getsize(file_A))\n",
    "        \n",
    "    for file_B in file_list_B:\n",
    "        B_size.append(os.path.getsize(file_B))    \n",
    "        \n",
    "    return(A_size,B_size)\n",
    "\n",
    "\n",
    "# KBを表示してくれる\n",
    "def look_datasize(folder_A):\n",
    "    file_list_A=glob.glob(folder_A)\n",
    "    A_size=[]\n",
    "    for file_A in file_list_A:\n",
    "        A_size.append(0.001*round(os.path.getsize(file_A),-3))\n",
    "\n",
    "    return(A_size)\n",
    "\n",
    "\n",
    "look_datasize(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\Abe_speech2\\Prep/*.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(fname):\n",
    "    Matubi=[]\n",
    "    with open(fname,errors='ignore') as data_file:\n",
    "                ftitle, fext = os.path.splitext(fname)\n",
    "                fname_out=ftitle+'Prep.txt'\n",
    "                #空白文字を削除する\n",
    "                text_data=data_file.read()\n",
    "                text_data=re.sub('.+　','',text_data)\n",
    "                #改行で区切る\n",
    "                text_data=re.sub('\\n','',text_data)\n",
    "                #句点で区切り、最後の要素を抜き取る\n",
    "                text_data=text_data.split('。')\n",
    "                with open(fname_out, mode='w',errors='ignore') as out_file:\n",
    "                    for i,text in enumerate(text_data):\n",
    "                        #読点以前を削除する\n",
    "    #                     text=text.split('、')\n",
    "                        #なぜか先頭に空白文字が入るので取り除く↓。\n",
    "                        text=text.lstrip()\n",
    "                        #読点以降のみが集まる↓\n",
    "                        text+='。'\n",
    "                        out_file.write(text)\n",
    "                    \n",
    "\n",
    "# new_mecab (r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\Abe_speech\\2012-10-01.txt')    \n",
    "\n",
    "# for file in glob.glob(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\Abe_speech2/*'):\n",
    "    Preprocessing(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in glob.glob(r'C:/Users/icech/Desktop/share/Lab/2018_09_05/Docments/low_aproval_rate/*'):\n",
    "#     ftitle, fext = os.path.splitext(file)\n",
    "#     fname_out=ftitle+'.mecab'\n",
    "#     to_mecab(file,fname_out)\n",
    "    \n",
    "# folder_list=['late_plus','late_minus','current_plus','current_','','',]    \n",
    "    \n",
    "# for file in glob.glob(r'C:/Users/icech/Desktop/share/Lab/2018_09_05/Docments/high_aproval_rate/*'):\n",
    "#     ftitle, fext = os.path.splitext(file)\n",
    "#     fname_out=ftitle+'.mecab'\n",
    "#     to_mecab(file,fname_out)    \n",
    "\n",
    "# folders=glob.glob(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\09_19\\*')\n",
    "\n",
    "#孫クラスのファイルをMecab化する\n",
    "for folder in folders:\n",
    "    files=glob.glob(folder+r'\\*.txt')\n",
    "    for file in files:\n",
    "        ftitle, fext = os.path.splitext(file)\n",
    "        fname_out=ftitle+'.mecab'\n",
    "        to_mecab(file,fname_out) \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stopword():\n",
    "    f = open(\"../docments/stopword.txt\",\"r\")\n",
    "    list = []\n",
    "    for x in f:\n",
    "        list.append(x.rstrip(\"\\n\"))\n",
    "        #以下のようにしてしまうと、改行コードがlistに入ってしまうため注意\n",
    "        #list.append(x)\n",
    "    f.close()\n",
    "    return(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_count_morpheme(word_class):\n",
    "    for i,file in enumerate(glob.glob(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\high_aproval_rate\\Mecab\\*')):\n",
    "        copus=[]\n",
    "        print(i)\n",
    "        stopword=make_stopword()\n",
    "        #語数カウンター\n",
    "        for line in make_lines(file):\n",
    "            for morpheme in line:\n",
    "                if len(morpheme['base'])==1:# or morpheme['base'] in stopword\n",
    "                    continue\n",
    "                if morpheme['pos'] == word_class:\n",
    "                    word_counter.update([morpheme['base']])\n",
    "                    #表層型をリスト型にする\n",
    "                    copus.append(morpheme['base'])\n",
    "                    copus.append(' ')\n",
    "        copus=','.join(copus)\n",
    "    return(copus,word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#品詞ごとに語数をカウントするコード\n",
    "# concatを使うことによって異なる長さのdfを結合している\n",
    "\n",
    "copus_dict={}\n",
    "word_classes=['形容詞','動詞','名詞','接続詞']\n",
    "df2=pd.DataFrame()\n",
    "for word_class in word_classes:\n",
    "    df1=pd.DataFrame()\n",
    "    word_counter = Counter()\n",
    "#         copus_dict[word_class+'word']= new_count_morpheme(word_class,file)[0]\n",
    "    copus_dict[word_class]= new_count_morpheme(word_class)[1]\n",
    "    count_word ,count_cnt =zip(*copus_dict[word_class].most_common())\n",
    "    df1[word_class]=count_word\n",
    "    df1[word_class+'_cnt']=count_cnt\n",
    "    df2=pd.concat([df2,df1],axis=1)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df2)\n",
    "df2.to_csv('./high_nostop.csv',encoding='shift_jis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここから下は使ってません！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_word ,verb_cnt =zip(*copus_dict['動詞'].most_common())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#mecabファイルを読み込んで語数を数えるコード\n",
    "df1=pd.DataFrame()\n",
    "for i,file in enumerate(glob.glob(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\low_aproval_rate\\Mecab\\*')):\n",
    "    month=re.sub( '.+Mecab.' ,'', file)\n",
    "    month=re.sub('-01Prep.+', '' ,month)\n",
    "    print(month)\n",
    "    \n",
    "    adjective_word,adjective_cnt=new_count_morpheme('形容詞',file)\n",
    "    verb_word,verb_cnt=new_count_morpheme('動詞',file)\n",
    "    noun_word,noun_cnt=new_count_morpheme('名詞',file)\n",
    "    conjuction_word,conjuction_cnt=new_count_morpheme('接続詞',file)\n",
    "\n",
    "    adjective_word ,adjective_cnt =zip(*adjective_cnt.most_common())\n",
    "    try:\n",
    "        verb_word ,verb_cnt =zip(*verb_cnt.most_common())\n",
    "    except:\n",
    "        pass   \n",
    "    \n",
    "    noun_word ,noun_cnt =zip(*noun_cnt.most_common())\n",
    "    conjuction_word,conjuction_cnt=zip(*conjuction_cnt.most_common())\n",
    "    \n",
    "    \n",
    "    adjective_data={'adj_cnt'.format(month):list(adjective_cnt),\n",
    "                    'adj_word':list(adjective_word)}\n",
    "    adjective_df=pd.DataFrame(adjective_data)\n",
    "\n",
    "    \n",
    "    verb_data={'verb_cnt':list(verb_cnt),\n",
    "               'verb_word':list(verb_word)}        \n",
    "    verb_df=pd.DataFrame(verb_data)\n",
    "\n",
    "    noun_data={'noun_cnt':list(noun_cnt),\n",
    "               'noun_word':list(noun_word)}\n",
    "#                'space':list(np.zeros(len(noun_cnt)))}\n",
    "    noun_df=pd.DataFrame(noun_data)\n",
    "    \n",
    "    conjuction_data={'conjuction_cnt':list(conjuction_cnt),\n",
    "               'conjuction_word':list(conjuction_word)}        \n",
    "    conjuction_df=pd.DataFrame(conjuction_data)\n",
    "    \n",
    "    \n",
    "    df2=pd.concat([adjective_df,verb_df,noun_df,conjuction_df],axis=1)\n",
    "    print(df2)\n",
    "    if i==0:\n",
    "        df1=df2.copy()\n",
    "    else:\n",
    "        df1=pd.concat([df1,df2])\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df3.head()\n",
    "print(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('./cmp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
