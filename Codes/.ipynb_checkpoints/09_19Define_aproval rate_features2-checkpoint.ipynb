{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このコードの目的\n",
    "\n",
    "支持率が伸びている/落ちているデータセットをサイズが同じになるように集積して、  \n",
    "特徴分析を行う  \n",
    "データセットが同じになるようにするとき、前処理した後のtxtファイルで比較するほうがよい\n",
    "+  \n",
    "機械学習にかける  \n",
    "\n",
    "追加した点は、globとFor文を使って、同じことを二階しないようにした"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from 過去のコード.Mecab_func import *\n",
    "import glob\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各フォルダのサイズを比較するコード\n",
    "#普通にエクスプローラーで見ればいい説\n",
    "\n",
    "\n",
    "\n",
    "def compare_datasize(folder_A,folder_B):\n",
    "    file_list_A=glob.glob(folder_A)\n",
    "    file_list_B=glob.glob(folder_B)    \n",
    "    A_size=[]\n",
    "    B_size=[]\n",
    "    \n",
    "    for file_A in file_list_A:\n",
    "        A_size.append(os.path.getsize(file_A))\n",
    "        \n",
    "    for file_B in file_list_B:\n",
    "        B_size.append(os.path.getsize(file_B))    \n",
    "        \n",
    "    return(A_size,B_size)\n",
    "\n",
    "\n",
    "# KBを表示してくれる\n",
    "def look_datasize(folder_A):\n",
    "    file_list_A=glob.glob(folder_A)\n",
    "    A_size=[]\n",
    "    for file_A in file_list_A:\n",
    "        A_size.append(0.001*round(os.path.getsize(file_A),-3))\n",
    "\n",
    "    return(A_size)\n",
    "\n",
    "\n",
    "look_datasize(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\Abe_speech2\\Prep/*.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(fname):\n",
    "    Matubi=[]\n",
    "    with open(fname,errors='ignore') as data_file:\n",
    "                ftitle, fext = os.path.splitext(fname)\n",
    "                fname_out=ftitle+'Prep.txt'\n",
    "                #空白文字を削除する\n",
    "                text_data=data_file.read()\n",
    "                text_data=re.sub('.+　','',text_data)\n",
    "                #改行で区切る\n",
    "                text_data=re.sub('\\n','',text_data)\n",
    "                #句点で区切り、最後の要素を抜き取る\n",
    "                text_data=text_data.split('。')\n",
    "                with open(fname_out, mode='w',errors='ignore') as out_file:\n",
    "                    for i,text in enumerate(text_data):\n",
    "                        #読点以前を削除する\n",
    "    #                     text=text.split('、')\n",
    "                        #なぜか先頭に空白文字が入るので取り除く↓。\n",
    "                        text=text.lstrip()\n",
    "                        #読点以降のみが集まる↓\n",
    "                        text+='。'\n",
    "                        out_file.write(text)\n",
    "                    \n",
    "\n",
    "# new_mecab (r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\Abe_speech\\2012-10-01.txt')    \n",
    "\n",
    "# for file in glob.glob(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\Abe_speech2/*'):\n",
    "    Preprocessing(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file in glob.glob(r'C:/Users/icech/Desktop/share/Lab/2018_09_05/Docments/low_aproval_rate/*'):\n",
    "#     ftitle, fext = os.path.splitext(file)\n",
    "#     fname_out=ftitle+'.mecab'\n",
    "#     to_mecab(file,fname_out)\n",
    "    \n",
    "# folder_list=['late_plus','late_minus','current_plus','current_','','',]    \n",
    "    \n",
    "# for file in glob.glob(r'C:/Users/icech/Desktop/share/Lab/2018_09_05/Docments/high_aproval_rate/*'):\n",
    "#     ftitle, fext = os.path.splitext(file)\n",
    "#     fname_out=ftitle+'.mecab'\n",
    "#     to_mecab(file,fname_out)    \n",
    "\n",
    "folders=glob.glob(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\09_19\\*')\n",
    "\n",
    "#孫クラスのファイルをMecab化する\n",
    "for folder in folders:\n",
    "    files=glob.glob(folder+r'\\*.txt')\n",
    "    for file in files:\n",
    "        ftitle, fext = os.path.splitext(file)\n",
    "        fname_out=ftitle+'.mecab'\n",
    "        to_mecab(file,fname_out) \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stopword():\n",
    "    f = open(\"../docments/stopword.txt\",\"r\")\n",
    "    list = []\n",
    "    for x in f:\n",
    "        list.append(x.rstrip(\"\\n\"))\n",
    "        #以下のようにしてしまうと、改行コードがlistに入ってしまうため注意\n",
    "        #list.append(x)\n",
    "    f.close()\n",
    "    return(list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_count_morpheme(word_class,folder_path):\n",
    "    for i,file in enumerate(glob.glob(folder_path+r'\\*.mecab')):\n",
    "        if i==9:\n",
    "            print('next_folder')\n",
    "            \n",
    "        stopword=make_stopword()\n",
    "        #語数カウンター\n",
    "        for line in make_lines(file):\n",
    "            for morpheme in line:\n",
    "                if len(morpheme['base'])==1 or morpheme['base'] in stopword:\n",
    "                    continue\n",
    "                if morpheme['pos'] == word_class:\n",
    "                    word_counter.update([morpheme['base']])\n",
    "    return(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\icech\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: generator 'make_lines' raised StopIteration\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_folder\n",
      "next_folder\n",
      "next_folder\n",
      "next_folder\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-a135c87cd74d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mword_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#         copus_dict[word_class+'word']= new_count_morpheme(word_class,file)[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mcopus_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnew_count_morpheme\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mcount_word\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcount_cnt\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcopus_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdf_tmp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_class\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcount_word\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-97ce012635f3>\u001b[0m in \u001b[0;36mnew_count_morpheme\u001b[1;34m(word_class, folder_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mstopword\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_stopword\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;31m#語数カウンター\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmake_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmorpheme\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmorpheme\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'base'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m# or morpheme['base'] in stopword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\share\\Lab\\2018_09_05\\Codes\\過去のコード\\Mecab_func.py\u001b[0m in \u001b[0;36mmake_lines\u001b[1;34m(fname_parsed)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m             \u001b[1;31m# 表層形はtab区切り、それ以外は','区切りでバラす\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m     \u001b[1;31m# 区切りがなければ終了\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#品詞ごとに語数をカウントするコード\n",
    "# concatを使うことによって異なる長さのdfを結合している\n",
    "\n",
    "copus_dict={}\n",
    "word_classes=['形容詞','動詞','名詞','接続詞']\n",
    "df2=pd.DataFrame()\n",
    "for word_class in word_classes:\n",
    "    df1=pd.DataFrame()\n",
    "    word_counter = Counter()\n",
    "#         copus_dict[word_class+'word']= new_count_morpheme(word_class,file)[0]\n",
    "    copus_dict[word_class]= new_count_morpheme(word_class)[1]\n",
    "    count_word ,count_cnt =zip(*copus_dict[word_class].most_common())\n",
    "    df1[word_class]=count_word\n",
    "    df1[word_class+'_cnt']=count_cnt\n",
    "    df2=pd.concat([df2,df1],axis=1)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#品詞ごとに語数をカウントするコード\n",
    "# concatを使うことによって異なる長さのdfを結合している\n",
    "\n",
    "copus_dict={}\n",
    "word_classes=['形容詞','動詞','名詞','接続詞']\n",
    "count_df=pd.DataFrame()\n",
    "folders=glob.glob(r'C:\\Users\\icech\\Desktop\\share\\Lab\\2018_09_05\\Docments\\09_19\\*')\n",
    "\n",
    "for folder in folders:\n",
    "    for word_class in word_classes:\n",
    "        df_tmp=pd.DataFrame()\n",
    "        word_counter = Counter()\n",
    "        tmp=new_count_morpheme(word_class,folder).most_common(200)\n",
    "        count_word ,count_cnt =zip(*new_count_morpheme(word_class,folder).most_common())\n",
    "        df_tmp[word_class]=count_word\n",
    "        df_tmp[word_class+'_cnt']=count_cnt\n",
    "        count_df=pd.concat([count_df,df_tmp],axis=1)\n",
    "        print(count_df)\n",
    "#         print(tmp)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df.to_csv('../stop_count_df.csv',encoding='shift_jis')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
